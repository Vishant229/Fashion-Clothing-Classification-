
Project :
Fashion Clothing Classification
Description
Fashion training set consists of 70,000 images divided into 60,000 training and 10,000 testing samples. Dataset sample consists of 28x28 grayscale image, associated with a label from 10 classes.

The 10 classes are as follows:
0 = T-shirt/top
1 = Trouser
2 = Pullover
3 = Dress
4 = Coat
5 = Sandal
6 = Shirt
7 = Sneaker
8 = Bag
9 = Ankle boot

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.

Importing the dataset
In [1]:
# Importing required libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
In [2]:
# Let's create training and testing dataframes using CSV files

training_df = pd.read_csv('fashion-mnist_train.csv')
testing_df = pd.read_csv('fashion-mnist_test.csv')
Visualizing the data
In [3]:
# Let's check first 5 rows of training dataframe

training_df.head()
Out[3]:
label	pixel1	pixel2	pixel3	pixel4	pixel5	pixel6	pixel7	pixel8	pixel9	...	pixel775	pixel776	pixel777	pixel778	pixel779	pixel780	pixel781	pixel782	pixel783	pixel784
0	2	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
1	9	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
2	6	0	0	0	0	0	0	0	5	0	...	0	0	0	30	43	0	0	0	0	0
3	0	0	0	0	1	2	0	0	0	0	...	3	0	0	0	0	1	0	0	0	0
4	3	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
5 rows × 785 columns

In [4]:
# Let's check last 5 rows of training dataframe

training_df.tail()
Out[4]:
label	pixel1	pixel2	pixel3	pixel4	pixel5	pixel6	pixel7	pixel8	pixel9	...	pixel775	pixel776	pixel777	pixel778	pixel779	pixel780	pixel781	pixel782	pixel783	pixel784
59995	9	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
59996	1	0	0	0	0	0	0	0	0	0	...	73	0	0	0	0	0	0	0	0	0
59997	8	0	0	0	0	0	0	0	0	0	...	160	162	163	135	94	0	0	0	0	0
59998	8	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
59999	7	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
5 rows × 785 columns

Observations
There are 784 columns with various pixel values of different images
First column (i.e. label) is the column to be predicted. We will ignore the same initially.
There are 60000 observations in training dataset
All pixel values are between 0 to 255
In [5]:
# Let's check first 5 rows of testing dataframe

testing_df.head()
Out[5]:
label	pixel1	pixel2	pixel3	pixel4	pixel5	pixel6	pixel7	pixel8	pixel9	...	pixel775	pixel776	pixel777	pixel778	pixel779	pixel780	pixel781	pixel782	pixel783	pixel784
0	0	0	0	0	0	0	0	0	9	8	...	103	87	56	0	0	0	0	0	0	0
1	1	0	0	0	0	0	0	0	0	0	...	34	0	0	0	0	0	0	0	0	0
2	2	0	0	0	0	0	0	14	53	99	...	0	0	0	0	63	53	31	0	0	0
3	2	0	0	0	0	0	0	0	0	0	...	137	126	140	0	133	224	222	56	0	0
4	3	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
5 rows × 785 columns

In [6]:
# Let's check last 5 rows of testing dataframe

testing_df.tail()
Out[6]:
label	pixel1	pixel2	pixel3	pixel4	pixel5	pixel6	pixel7	pixel8	pixel9	...	pixel775	pixel776	pixel777	pixel778	pixel779	pixel780	pixel781	pixel782	pixel783	pixel784
9995	0	0	0	0	0	0	0	0	0	0	...	32	23	14	20	0	0	1	0	0	0
9996	6	0	0	0	0	0	0	0	0	0	...	0	0	0	2	52	23	28	0	0	0
9997	8	0	0	0	0	0	0	0	0	0	...	175	172	172	182	199	222	42	0	1	0
9998	8	0	1	3	0	0	0	0	0	0	...	0	0	0	0	0	1	0	0	0	0
9999	1	0	0	0	0	0	0	0	140	119	...	111	95	75	44	1	0	0	0	0	0
5 rows × 785 columns

Observations
There are 784 columns with various pixel values of different images
There are 10000 observations in testing dataset
All pixel values are between 0 to 255
In [7]:
# Let's create numpy arrays for training and testing dataframes 
# using float values for further processing

trainData = np.array(training_df, dtype='float32')
testData = np.array(testing_df, dtype='float32')
In [8]:
# Let's check the training data and it's shape

print(trainData)
print('\n')
print(trainData.shape)
[[2. 0. 0. ... 0. 0. 0.]
 [9. 0. 0. ... 0. 0. 0.]
 [6. 0. 0. ... 0. 0. 0.]
 ...
 [8. 0. 0. ... 0. 0. 0.]
 [8. 0. 0. ... 0. 0. 0.]
 [7. 0. 0. ... 0. 0. 0.]]


(60000, 785)
In [9]:
# Let's check the testing data

print(testData)
print('\n')
print(testData.shape)
[[0. 0. 0. ... 0. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 [2. 0. 0. ... 0. 0. 0.]
 ...
 [8. 0. 0. ... 0. 1. 0.]
 [8. 0. 1. ... 0. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]]


(10000, 785)
In [10]:
# Let's visualize an image randomly using any random number from training data

# Importing library random to get any random number

import random
i = random.randint(1, trainData.shape[0])

# Displaying a random image using pixel values(reshaped)

plt.imshow(trainData[i, 1:].reshape(28, 28))
print(trainData[i, 0])

# Let's check all class values with labels for reference

# 0 = T-shirt/top
# 1 = Trouser
# 2 = Pullover
# 3 = Dress
# 4 = Coat
# 5 = Sandal
# 6 = Shirt
# 7 = Sneaker
# 8 = Bag
# 9 = Ankle boot
2.0

In [11]:
# Let's view multiple images in a grid format

Grid_W = 10 # Number of columns
Grid_H = 10 # Number of rows

# Let's create subplot to display multiple images

fig, axes = plt.subplots(nrows=Grid_H, ncols=Grid_W, figsize = (18,18))
axes = axes.ravel()

# Let's loop through the training dataset to display random 100 images

for i in np.arange(0, Grid_W*Grid_H):
    index = np.random.randint(0, len(trainData))
    axes[i].imshow(trainData[index, 1:].reshape(28, 28))
    axes[i].set_title(trainData[index, 0])
    axes[i].axis('off')

Splitting and reshaping the data
In [12]:
# Let's split the data into initial X_train and y_train data
# Normalizing each pixel value by dividing the same with 255
# i.e. Converting all pixel values between range 0 to 1

X_train_init = trainData[:, 1:] / 255 
y_train_init = trainData[:, 0]

# Let's do the same for test data

X_test = testData[:, 1:] / 255 
y_test = testData[:, 0]
In [13]:
# Let's split the training data further to create training and validation data

# Importing necessary library

from sklearn.model_selection import train_test_split
X_train, X_valid, y_train, y_valid = train_test_split(X_train_init, y_train_init, 
                                                      test_size=0.2, random_state = 0)
In [14]:
X_train.shape
Out[14]:
(48000, 784)
In [15]:
X_valid.shape
Out[15]:
(12000, 784)
In [16]:
# Let's reshape the array similar to an image data e.g. (row, column, channel)

X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_valid = X_valid.reshape(X_valid.shape[0], 28, 28, 1)
In [17]:
# Let's check the new shape of the training, testing and validation data

X_train.shape
Out[17]:
(48000, 28, 28, 1)
In [18]:
X_test.shape
Out[18]:
(10000, 28, 28, 1)
In [19]:
X_valid.shape
Out[19]:
(12000, 28, 28, 1)
Training a Neural Network Model
In [20]:
# Importing necessary libraries using keras to build a NN

import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.optimizers import adam
Using TensorFlow backend.
In [21]:
# Let's create a NN model

# Initialization of model

cnnModel = Sequential()

# Adding Convolution layer

cnnModel.add(Conv2D(filters=64, kernel_size=(3,3), input_shape = (28,28,1), activation = 'relu'))

# Adding Pooling layer

cnnModel.add(MaxPooling2D(pool_size=(2,2)))

# Let's apply dropout

cnnModel.add(Dropout(0.2))

# Flattening the model output

cnnModel.add(Flatten())

# Adding a dense fully connected layer

cnnModel.add(Dense(units=64, activation = 'relu'))
cnnModel.add(Dense(units=32, activation = 'relu'))
cnnModel.add(Dense(units=10, activation = 'sigmoid'))
WARNING:tensorflow:From C:\Users\sushant.dhumak\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\sushant.dhumak\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
In [22]:
# Let's compile the model

cnnModel.compile(optimizer=adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
In [23]:
# Let's fit the model to validation data

cnnModel.fit(x=X_train, 
             y=y_train, 
             batch_size=512, 
             epochs=50, 
             verbose=1, 
             validation_data=(X_valid, y_valid)
            )
WARNING:tensorflow:From C:\Users\sushant.dhumak\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 48000 samples, validate on 12000 samples
Epoch 1/50
48000/48000 [==============================] - 29s 603us/step - loss: 0.8694 - acc: 0.6860 - val_loss: 0.5105 - val_acc: 0.8151
Epoch 2/50
48000/48000 [==============================] - 28s 593us/step - loss: 0.4635 - acc: 0.8352 - val_loss: 0.4250 - val_acc: 0.8485
Epoch 3/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.4047 - acc: 0.8577 - val_loss: 0.3789 - val_acc: 0.8688
Epoch 4/50
48000/48000 [==============================] - 29s 599us/step - loss: 0.3647 - acc: 0.8706 - val_loss: 0.3512 - val_acc: 0.8774
Epoch 5/50
48000/48000 [==============================] - 29s 599us/step - loss: 0.3393 - acc: 0.8806 - val_loss: 0.3707 - val_acc: 0.8667
Epoch 6/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.3201 - acc: 0.8869 - val_loss: 0.3361 - val_acc: 0.8843
Epoch 7/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.3018 - acc: 0.8933 - val_loss: 0.3157 - val_acc: 0.8901
Epoch 8/50
48000/48000 [==============================] - 29s 598us/step - loss: 0.2888 - acc: 0.8990 - val_loss: 0.3044 - val_acc: 0.8936
Epoch 9/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.2761 - acc: 0.9015 - val_loss: 0.3060 - val_acc: 0.8920
Epoch 10/50
48000/48000 [==============================] - 29s 596us/step - loss: 0.2661 - acc: 0.9045 - val_loss: 0.2869 - val_acc: 0.8988
Epoch 11/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.2643 - acc: 0.9043 - val_loss: 0.2805 - val_acc: 0.9017
Epoch 12/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.2539 - acc: 0.9094 - val_loss: 0.2786 - val_acc: 0.9026
Epoch 13/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.2453 - acc: 0.9110 - val_loss: 0.2699 - val_acc: 0.9057
Epoch 14/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.2326 - acc: 0.9165 - val_loss: 0.2721 - val_acc: 0.9008
Epoch 15/50
48000/48000 [==============================] - 28s 590us/step - loss: 0.2256 - acc: 0.9197 - val_loss: 0.2724 - val_acc: 0.9039
Epoch 16/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.2238 - acc: 0.9193 - val_loss: 0.2665 - val_acc: 0.9068
Epoch 17/50
48000/48000 [==============================] - 29s 598us/step - loss: 0.2145 - acc: 0.9220 - val_loss: 0.2584 - val_acc: 0.9104
Epoch 18/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.2108 - acc: 0.9236 - val_loss: 0.2731 - val_acc: 0.9026
Epoch 19/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.2071 - acc: 0.9249 - val_loss: 0.2624 - val_acc: 0.9080
Epoch 20/50
48000/48000 [==============================] - 30s 631us/step - loss: 0.1979 - acc: 0.9288 - val_loss: 0.2553 - val_acc: 0.9102
Epoch 21/50
48000/48000 [==============================] - 30s 621us/step - loss: 0.1938 - acc: 0.9296 - val_loss: 0.2587 - val_acc: 0.9120
Epoch 22/50
48000/48000 [==============================] - 30s 615us/step - loss: 0.1855 - acc: 0.9332 - val_loss: 0.2523 - val_acc: 0.9104
Epoch 23/50
48000/48000 [==============================] - 32s 666us/step - loss: 0.1833 - acc: 0.9343 - val_loss: 0.2520 - val_acc: 0.9147
Epoch 24/50
48000/48000 [==============================] - 31s 640us/step - loss: 0.1783 - acc: 0.9356 - val_loss: 0.2616 - val_acc: 0.9078
Epoch 25/50
48000/48000 [==============================] - 30s 633us/step - loss: 0.1758 - acc: 0.9366 - val_loss: 0.2625 - val_acc: 0.9075
Epoch 26/50
48000/48000 [==============================] - 30s 626us/step - loss: 0.1715 - acc: 0.9383 - val_loss: 0.2749 - val_acc: 0.9047
Epoch 27/50
48000/48000 [==============================] - 30s 629us/step - loss: 0.1714 - acc: 0.9368 - val_loss: 0.2562 - val_acc: 0.9098
Epoch 28/50
48000/48000 [==============================] - 30s 620us/step - loss: 0.1618 - acc: 0.9414 - val_loss: 0.2578 - val_acc: 0.9118
Epoch 29/50
48000/48000 [==============================] - 30s 629us/step - loss: 0.1597 - acc: 0.9423 - val_loss: 0.2641 - val_acc: 0.9103
Epoch 30/50
48000/48000 [==============================] - 30s 626us/step - loss: 0.1547 - acc: 0.9436 - val_loss: 0.2586 - val_acc: 0.9118
Epoch 31/50
48000/48000 [==============================] - 30s 616us/step - loss: 0.1522 - acc: 0.9445 - val_loss: 0.2734 - val_acc: 0.9100
Epoch 32/50
48000/48000 [==============================] - 29s 596us/step - loss: 0.1454 - acc: 0.9480 - val_loss: 0.2569 - val_acc: 0.9158
Epoch 33/50
48000/48000 [==============================] - 29s 602us/step - loss: 0.1461 - acc: 0.9467 - val_loss: 0.2519 - val_acc: 0.9163
Epoch 34/50
48000/48000 [==============================] - 28s 594us/step - loss: 0.1384 - acc: 0.9500 - val_loss: 0.2503 - val_acc: 0.9173
Epoch 35/50
48000/48000 [==============================] - 29s 598us/step - loss: 0.1363 - acc: 0.9500 - val_loss: 0.2680 - val_acc: 0.9103
Epoch 36/50
48000/48000 [==============================] - 29s 598us/step - loss: 0.1388 - acc: 0.9496 - val_loss: 0.2573 - val_acc: 0.9178
Epoch 37/50
48000/48000 [==============================] - 29s 596us/step - loss: 0.1310 - acc: 0.9529 - val_loss: 0.2591 - val_acc: 0.9144
Epoch 38/50
48000/48000 [==============================] - 29s 597us/step - loss: 0.1260 - acc: 0.9545 - val_loss: 0.2641 - val_acc: 0.9138
Epoch 39/50
48000/48000 [==============================] - 29s 600us/step - loss: 0.1245 - acc: 0.9549 - val_loss: 0.2644 - val_acc: 0.9128
Epoch 40/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.1255 - acc: 0.9548 - val_loss: 0.2618 - val_acc: 0.9162
Epoch 41/50
48000/48000 [==============================] - 29s 600us/step - loss: 0.1186 - acc: 0.9568 - val_loss: 0.2622 - val_acc: 0.9159
Epoch 42/50
48000/48000 [==============================] - 29s 601us/step - loss: 0.1151 - acc: 0.9592 - val_loss: 0.2578 - val_acc: 0.9157
Epoch 43/50
48000/48000 [==============================] - 28s 593us/step - loss: 0.1154 - acc: 0.9581 - val_loss: 0.2653 - val_acc: 0.9158
Epoch 44/50
48000/48000 [==============================] - 30s 619us/step - loss: 0.1119 - acc: 0.9590 - val_loss: 0.2754 - val_acc: 0.9120
Epoch 45/50
48000/48000 [==============================] - 30s 617us/step - loss: 0.1095 - acc: 0.9603 - val_loss: 0.2714 - val_acc: 0.9162
Epoch 46/50
48000/48000 [==============================] - 29s 607us/step - loss: 0.1045 - acc: 0.9623 - val_loss: 0.2691 - val_acc: 0.9152
Epoch 47/50
48000/48000 [==============================] - 29s 595us/step - loss: 0.1061 - acc: 0.9618 - val_loss: 0.2694 - val_acc: 0.9164
Epoch 48/50
48000/48000 [==============================] - 29s 600us/step - loss: 0.1052 - acc: 0.9612 - val_loss: 0.2696 - val_acc: 0.9144
Epoch 49/50
48000/48000 [==============================] - 29s 598us/step - loss: 0.0990 - acc: 0.9648 - val_loss: 0.2817 - val_acc: 0.9139
Epoch 50/50
48000/48000 [==============================] - 29s 599us/step - loss: 0.0973 - acc: 0.9638 - val_loss: 0.2865 - val_acc: 0.9120
Out[23]:
<keras.callbacks.History at 0x17554bb2f98>
Evaluating the Model
In [24]:
# Checking the accuracy of model on test data

accuracy = cnnModel.evaluate(X_test, y_test)
print('Test Acuuracy : {}'.format(accuracy[1]))
10000/10000 [==============================] - 1s 140us/step
Test Acuuracy : 0.9183
In [27]:
# Getting the prediction classes for test data

prediction_classes = cnnModel.predict_classes(X_test)
In [28]:
# Let's visualize some of the predicted classes with it's true classes

Grid_W = 6
Grid_H = 6

fig, axes = plt.subplots(Grid_H, Grid_W, figsize=(20,20))

axes = axes.ravel()

for i in np.arange(0, Grid_H*Grid_W):
    axes[i].imshow(X_test[i].reshape(28,28))
    axes[i].set_title('Predicted = {:0.1f} \n Actual = {:0.1f}'.format(prediction_classes[i], 
                                                                       y_test[i]))
    axes[i].axis('off')

In [30]:
#Let's generate a claasification report 

from sklearn.metrics import classification_report

classes = 10
targets = ["Class {}".format(i) for i in range(classes)]

print(classification_report(y_test, prediction_classes, target_names = targets))
              precision    recall  f1-score   support

     Class 0       0.82      0.91      0.86      1000
     Class 1       0.98      0.98      0.98      1000
     Class 2       0.85      0.90      0.87      1000
     Class 3       0.94      0.91      0.92      1000
     Class 4       0.89      0.88      0.88      1000
     Class 5       0.99      0.97      0.98      1000
     Class 6       0.83      0.71      0.77      1000
     Class 7       0.95      0.97      0.96      1000
     Class 8       0.98      0.98      0.98      1000
     Class 9       0.97      0.97      0.97      1000

   micro avg       0.92      0.92      0.92     10000
   macro avg       0.92      0.92      0.92     10000
weighted avg       0.92      0.92      0.92     10000

